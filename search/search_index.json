{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MAMSI","text":"<p>MAMSI is a Python framework designed for the integration of multi-assay mass spectrometry datasets.  In addition, the MAMSI framework provides a platform for linking statistically significant features of untargeted multi-assay liquid chromatography \u2013 mass spectrometry (LC-MS) metabolomics datasets into clusters defined by their structural properties based on mass-to-charge ratio (m/z) and retention time (RT).</p> <p>N.B. the framework was tested on metabolomics phenotyping data, but it should be usable with other types of LC-MS data.</p>"},{"location":"#overview","title":"Overview","text":""},{"location":"#features","title":"Features","text":"<ul> <li>Data integration analysis using the Multi-Block Partial Least Squares (MB-PLS) [1] algorithm. The <code>MamsiPls</code> class inherits from the <code>mbpls</code> package [1]. For more information on MB-PLS, please visit mbpls Documentation.</li> <li>Multi-Block Variable Importance in Projection (MB-VIP) [2].</li> <li>Estimation of statistically significant features (variables) using MB-VIP and permutation testing.</li> <li>Linking significant features into clusters defined by structural properties of metabolites.</li> <li>Feature network links.</li> <li>Annotation of untargeted LC-MS features (only supported for assays analysed by the National Phenome Centre).</li> </ul>"},{"location":"#sources-and-materials","title":"Sources and Materials","text":"<p>The package source code is accessible via GitHub at https://github.com/kopeckylukas/py-mamsi</p> <p>Training materials including sample data can be found at https://github.com/kopeckylukas/py-mamsi-tutorials.</p>"},{"location":"#installation","title":"Installation","text":""},{"location":"#dependencies","title":"Dependencies","text":"<ul> <li>mbpls==1.0.4</li> <li>pandas</li> <li>numpy</li> <li>matplotlib</li> <li>scipy</li> <li>scikit-learn</li> <li>seaborn</li> <li>networkx</li> <li>pyvis</li> </ul>"},{"location":"#user-installation","title":"User Installation","text":"<p>You can install MAMSI from source code given you have installed both Python (&gt;=3.9) and PIP.</p> <p>First, clone the repository from GitHub to your computer. You can use following commands if you have a version of Git installed on your computer:</p> <pre><code>git https://github.com/kopeckylukas/py-mamsi\n\ncd py-mamsi\n</code></pre> <p>When you are in the cloned project folder, type the following code to install MAMSI and all dependencies:  <pre><code>pip install .\n</code></pre></p> <p>Alternatively, you can install dependencies using pip and MAMSI using Python: <pre><code>pip install -r requirements.txt\npython setup.py develop\n</code></pre></p>"},{"location":"#issues-and-collaboration","title":"Issues and Collaboration","text":"<p>Thank you for supporting the MAMSI project. MAMSI is an open-source software and welcomes any form of contribution and support.</p>"},{"location":"#issues","title":"Issues","text":"<p>Please submit any bugs or issues via the project's GitHub issue page and any include details about the (<code>mamsi.__version__</code>) together with any relevant input data/metadata. </p>"},{"location":"#collaboration","title":"Collaboration","text":""},{"location":"#pull-requests","title":"Pull requests","text":"<p>You can actively collaborate on MAMSI package by submitting any changes via a pull request. All pull requests will be reviewed by the MAMSI team and merged in due course. </p>"},{"location":"#contributions","title":"Contributions","text":"<p>If you would like to become a contributor on the MAMSI project, please contact Lukas Kopecky.</p>"},{"location":"#acknowledgement","title":"Acknowledgement","text":"<p>This package was developed as part of Lukas Kopecky's PhD project at Imperial College London, funded by Waters UK. It is free to use, published under BSD 3-Clause licence.</p> <p>The authors of this package would like to acknowledge the authors of the mbpls package [1] which became the backbone of MAMSI. For more information on MB-PLS, please visit MB-PLS Documentation.</p> <p>Further, we would like to thank Prof Simon Lovestone and Dr Shivani Misra for allowing us to use their data, AddNeuroMed [3] and MY Diabetes [5] respectively, for the development of this package. </p>"},{"location":"#citing-us","title":"Citing us","text":"<p>If you use MAMSI in a scientific publication, we would appreciate citations. The MAMSI publication is currently under the review process. </p>"},{"location":"#references","title":"References","text":"<p>[1] A. Baum and L. Vermue, \"Multiblock PLS: Block dependent prediction modeling for Python,\" J. Open Source Softw., vol. 4, no. 34, 2019, doi: 10.21105/joss.01190.</p> <p>[2] C. Wieder et al., \"PathIntegrate: Multivariate modelling approaches for pathway-based multi-omics data integration,\" PLOS Comput. Biol., vol. 20, no. 3, p. e1011814, Mar 2024, doi: 10.1371/journal.pcbi.1011814.</p> <p>[3] S. Lovestone et al., \"AddNeuroMed\u2014The European Collaboration for the Discovery of Novel Biomarkers for Alzheimer's Disease,\" Ann. N. Y. Acad. Sci, vol. 1180, no. 1, pp. 36-46, 2009, doi: 10.1111/j.1749-6632.2009.05064.x.</p> <p>[4] A. M. Wolfer et al., \"peakPantheR, an R package for large-scale targeted extraction and integration of annotated metabolic features in LC\u2013MS profiling datasets,\" Bioinformatics, vol. 37, no. 24, pp. 4886-4888, 2021, doi: 10.1093/bioinformatics/btab433.</p> <p>[5] S. Misra et al., \"Systematic screening for monogenic diabetes in people of South Asian and African Caribbean ethnicity: Preliminary results from the My Diabetes study,\" presented at the Diabet. Med., Mar 2018.</p> <p>[6]  M. Lewis et al., \u201cAn Open Platform for Large Scale LC-MS-Based Metabolomics ,\u201d ChemRxiv, 2022. doi: 10.26434/chemrxiv-2022-nq9k0.</p> <p>[7] C. A. Smith et al., \"XCMS:\u2009 Processing Mass Spectrometry Data for Metabolite Profiling Using Nonlinear Peak Alignment, Matching, and Identification,\" Anal. Chem., vol. 78, no. 3, pp. 779-787, Feb 2006, doi: 10.1021/ac051437y.</p> <p>[8] C. J. Sands et al., \"The nPYc-Toolbox, a Python module for the pre-processing, quality-control and analysis of metabolic profiling datasets,\" Bioinformatics, vol. 35, no. 24, pp. 5359-5360, 2019, doi: 10.1093/bioinformatics/btz566.</p>"},{"location":"tutorials/","title":"Tutorials and Training Materials","text":""},{"location":"tutorials/#training-materials","title":"Training Materials","text":"<p>You can find all MAMSI training materials by visiting our MAMSI Tutorials repository. </p> <p>The <code>MamsiPls</code> class inherits from the <code>mbpls</code> package [1]. For more information on MB-PLS, please visit mbpls Documentation.</p>"},{"location":"tutorials/#tutorial-example-classification","title":"Tutorial Example - Classification","text":"<p>The Multi-Assay Mass Spectrometry Integration (MAMSI) workflow allows for integrative analysis of multiple metabolomics LC-MS assays data. The MAMSI workflow utilises a multi-block partial-least squares (MB-PLS) discriminant analysis algorithm, which allows for the integration of multiple assays and the subsequent identification of the most significant predictors (features). The identification of statistically significant predictors is done using a multi-block version of the variable importance in projection (MB-VIP) procedure coupled with permutation testing. This enables us to obtain empirical p-values for each feature across all assays. MAMSI also offers an easy interpretation of significant features. This is done by grouping of the significant features based on their structural properties (mass-to-charge ratio and retention time) and compared to their correlations. This can be visualised by a network plot.</p> <p>This notebook showcases the use of the MAMSI workflow for the prediction (classification) of the biological sex of patients within the AddNeuroMed cohort [3] - dataset of Alzheimer's disease patients. For this task, we will use 3 metabolomics blood serum assays. The assays were processed by the National Phenome Centre following the NPC protocol [6]. Subsequently, data were pre-processed using XCMS [7] and nPYc toolbox [8].</p> <p>Assays Overview </p> Assay Number of features Description HPOS 681 Hydrophilic interaction liquid chromatography (HILIC) positive ionisation LPOS 4,886 Lipidomic reversed  phase chromatography positive ionisation LNEG 2,091 Lipidomic reversed phase chromatography negative ionisation <p>Outcome variable: Biological Sex</p> Class Number of samples Male 283 Female 294"},{"location":"tutorials/#load-packages","title":"Load Packages","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom mamsi.mamsi_pls import MamsiPls\nfrom mamsi.mamsi_struct_search import MamsiStructSearch\nfrom matplotlib import pyplot as plt\n</code></pre>"},{"location":"tutorials/#load-sample-data","title":"Load Sample Data","text":"<p> Data used within this quickstart guide originate from the the AddNeuroMed [1] cohort - dataset of Alzheimer's disease patients.  You can download the sample data from this link.</p> <pre><code>metadata = pd.read_csv('../sample_data/alz_metadata.csv')\n# The PLS algorithm requires the response variable to be numeric. \n# We will encode the outcome \"Gender\" (Biological Sex) as 1 for female and 0 for male subjects. \ny = metadata[\"Gender\"].apply(lambda x: 1 if x == 'Female' else 0)\n\n# Import LC-MS data\n# Add prefix to the columns names. This will be crucial for interpreting the results later on.\nhpos = pd.read_csv('./sample_data/alz_hpos.csv').add_prefix('HPOS_')\nlpos = pd.read_csv('./sample_data/alz_lpos.csv').add_prefix('LPOS_')\nlneg = pd.read_csv('./sample_data/alz_lneg.csv').add_prefix('LNEG_')\n</code></pre>"},{"location":"tutorials/#split-data","title":"Split data","text":"<p>Split the dataset into training and testing subsets. The training subset will be used to fit the model, for cross-validation and for estimation of the number of latent variables. Then, the testing subset will be used as an independent dataset to assess the performance of the model. This will ensure that the model is not over-fitted to the data and that it can predict the outcome of the samples. </p> <pre><code># Split data in the ratio 90:10 for training and testing respectively.\nhpos_train, hpos_test, y_train, y_test = train_test_split(hpos, y, test_size=0.1, random_state=42)\n\n#\u00a0Split the other two blocks based on the indices of the hpos block.\nlpos_train = lpos.iloc[hpos_train.index,:]\nlneg_train = lneg.iloc[hpos_train.index,:]\n\nlpos_test = lpos.iloc[hpos_test.index,:]\nlneg_test = lneg.iloc[hpos_test.index,:]\n</code></pre>"},{"location":"tutorials/#fit-model","title":"Fit Model","text":""},{"location":"tutorials/#fit-mb-pls-model-and-estimate-lvs","title":"Fit MB-PLS Model and Estimate LVs","text":"<p>As an example, we will start by fitting a MB-PLS model (from the MamsiPls class) with 1 component/latent variable (LV) and using the standard scaler. As a result, we will obtain super scores, block loadings and scores and block importances. Note that MamsiPls inherits its behaviour from the mbpls [1] model which, by default, uses the NIPALS algorithm. To see all possible configurations for the MamsiPls and mbpls models, see the mbpls documentation.</p> <p>We can then estimate the number of latent variables in the model. The MamsiPls class provides method <code>MamsiPls.estimate_lv()</code>\u00a0to estimate number of LVs in the model using k-fold cross-validation (CV). The k-fold CV is repeated k-times corresponding to number of LVs in the most complex model. The lowest possible number of LVs where the model stabilised (model performance did not rise by adding more LVs) was selected as the final model.</p> <p>For the classification task, you can choose from <code>'auc', 'precision', 'recall', 'f1', 'accuracy'</code> metrics to perform the LV estimation on the mean value of validation CV splits. <pre><code>mamsipls = MamsiPls(n_components=1)\nmamsipls.fit([hpos_train, lpos_train, lneg_train], y_train)\n\n# Estimate the number of latent variables in you model\nmamsipls.estimate_lv([hpos_train, lpos_train, lneg_train], y_train, metric='auc')\n</code></pre> The LV estimation result shows that the model has 6 latent variables/components. Adding more LVs to the model could lead to overfitting. </p>"},{"location":"tutorials/#evaluate-final-model","title":"Evaluate Final Model","text":"<p>We can evaluate the performance of the model by predicting the outcome on an independent (testing) dataset that has not been used for model training. For this, we will use the 'testing' subset that we obtained during the train-test-split. </p> <p>We can get the performance scores by calling the <code>mamsipls.evaluate_class_model()</code> method.</p> <p><pre><code>predicted = mamsipls.evaluate_class_model([hpos_test, lpos_test, lneg_test], y_test.array)\n</code></pre> </p> Metric Score Accuracy 0.966 Recall 1.0 Specificity 0.943 F<sub>1</sub> Score 0.971 AUC 0.933 <p>The scores and confusion matrix above indicate that the model performance has improved. If we are happy with such model, we can start with model interpretation. </p>"},{"location":"tutorials/#estimate-feature-importance","title":"Estimate Feature Importance","text":"<p>We can start with reviewing the Multi-Block Variable Importance in Projection (MB-VIP) scores [2]. The MB-VIP metric is the sum (weighted by the amount of variance of Y explained by each respective component) of the squared weight values. It provides a summary of the importance of a variable accounting for all weight vectors. VIPs are bounded between 0 (no effect) and infinity. Because it is calculated from the weights w, for PLS models with a single component, these are directly proportional to w<sup>2</sup>. The VIP metric has the disadvantage of pooling together w vectors from components which contribute a very small magnitude to the model's R<sup>2</sup>Y.</p>"},{"location":"tutorials/#multiblock-variable-importance-in-projection","title":"Multiblock Variable Importance in Projection","text":"<p><pre><code>mb_vip = mamsipls.mb_vip(plot=True)\n</code></pre> </p> <p>Unfortunately, the assessment of variable importance in MB-PLS multivariate models is not straightforward, given the choice of parameters and their different interpretations, especially in models with more than 1 LV. To obtain a ranking of variables from the data matrix X associated with Y, we recommend using permutation testing coupled with the MB-VIP metric to estimate the empirical p-values for each variable.</p>"},{"location":"tutorials/#permutation-testing","title":"Permutation Testing","text":"<p>You can perform permutation testing using the <code>MamsiPls.mb_vip_permtest()</code> method. We recommend to perform at least 10,000 permutations, but ideally &gt;100,000 for a good p-value estimate. You can find pre-calculated p-values at link.</p> <p>We can review the empirical null distribution of the MB-VIP scores of a statistically non-significant feature (1) and compare it to a statically significant feature (5769).</p> <p>In both cases, the dashed line indicates the MB-VIP score for each feature of the observed (non-permuted) model. </p> <p>The empirical p-values for the i <sup>th</sup> feature are calculated by dividing the number of MB-VIP scores of the null (permuted) models for the i <sup>th</sup> feature higher than the observed MB-VIP score for i <sup>th</sup> feature, by the total number of permutations.</p> <p><pre><code>p_vals, null_vip = mamsipls.mb_vip_permtest([hpos, lpos, lneg], y, n_permutations=10000, return_scores=True)\n</code></pre> </p> <p>Note that the pre-calculated <code>null_vip</code> file contains MB-VIP scores for first 400 null models (permutations) only so the p-value displayed on the plot below does not correspond with the plot itself.</p>"},{"location":"tutorials/#interpret-statistically-significant-features","title":"Interpret Statistically Significant Features","text":"<p><pre><code># merge the LC-MS data into a single data frame\nx = pd.concat([hpos, lpos, lneg], axis=1)\n\n# Select features with p-value &lt; 0.01.\n# You can also apply multiple testing correction methods to adjust the p-value threshold.\nmask = np.where(p_vals &lt; 0.01)\nselected = x.iloc[:, mask[0]]\n</code></pre> You can use MAMSI Structural Search tool (<code>MamsiStructSearch()</code>) to help you understand the nature of statistically significant features. </p> <p>Firstly, all features are split into retention time (RT) windows of 5 seconds intervals, then each RT window is searched for isotopologue signatures by searching mass differences of 1.00335 Da between mass-to-charge ratios (m/z) of the features; if two or more features resemble a mass isotopologue signature then they are grouped together. This is followed by a search for common adduct signatures. This is achieved by calculating hypothetical neutral masses based on common adducts in electrospray ionisation. If hypothetical neutral masses match for two or more features within a pre-defined tolerance (15 ppm) then these features are grouped together. Overlapping adduct clusters and isotopologue clusters are then merged to form structural clusters. Further, we search cross-assay clusters using [M+H]<sup>+</sup>/[M-H]<sup>-</sup> as link references. Additionally, our structural search tool, that utilises region of interest (ROI) files from peakPantheR [4], allows for automated annotation of  some features based on the RT for a given chromatography and m/z.</p>"},{"location":"tutorials/#mamsi-structural-search-tool","title":"MAMSI Structural Search Tool","text":"<p><pre><code># First, we need to define the MamsiStructSearch object \n# and choose the tolerances for the retention time and m/z matching.\nstruct = MamsiStructSearch(rt_win=5, ppm=10)\n\n# Load Selected LC-MS Features\nstruct.load_lcms(selected)\n\nstruct.get_structural_clusters(annotate=True)\npd.set_option(\"display.max_rows\", 15)\n</code></pre> We can then perform the structural search.  Note, use the <code>annotate=True</code> to get the annotation for selected features  only if your LC-MS data originate from the National Phenome Centre or follow the NPC protocol.</p> <p>We also need to perform hierarchical correlation clustering between selected features.  You can choose either the silhouette method or define a straight cut-off on the dendrogram <pre><code>struct.get_correlation_clusters(flat_method='silhouette', max_clusters=11)\n</code></pre> Best number of clusters based on silhouette score: 8  Silhouette score for 8 clusters: 0.2436798413177305</p> <p> </p> <p>Finally we can visualise the structural relationships using a network plot.  The different node colours represent different flattened hierarchical correlation clusters, while the edges between nodes identify their structural links. You can also save the network as an NX object and review in Cytoscape to get better insight on what the structural relationship between individual features are (e.g. adduct links, isotopologues, cross-assay links). <pre><code>network = struct.get_structural_network(include_all=True, interactive=False, labels=True, return_nx_object=True)\n</code></pre> </p>"},{"location":"api_references/MamsiPls/","title":"MamsiPls","text":""},{"location":"api_references/MamsiPls/#mamsipls","title":"MamsiPls","text":"<p>source <pre><code>MamsiPls(\n   n_components = 2, full_svd = False, method = 'NIPALS', standardize = True,\n   max_tol = 1e-14, nipals_convergence_norm = 2, calc_all = True, sparse_data = False,\n   copy = True\n)\n</code></pre></p> <p>A class that extends the MB_PLS class by extra methods convenient in Chemometrics and Metabolomics research.  It is based on MB-PLS package: Baum et al., (2019). Multiblock PLS: Block dependent prediction modeling for Python. This wrapper has some extra methods convenient in Chemometrics and Metabolomics research.</p> <p>For a full list of methods, please refer to the MB-PLS class documentation.</p> <p>Args</p> <ul> <li>n_components (int, optional) : A number of Latent Variables (LV). Defaults to 2.</li> <li>full_svd (bool, optional) : Whether to use full singular value decomposition when performing the SVD method.      Set to False when using very large quadratic matrices (X). Defaults to False.</li> <li>method (str, optional) : The method used to derive the model attributes. Options are 'UNIPALS', 'NIPALS', 'SIMPLS',      and 'KERNEL'. Defaults to 'NIPALS'.</li> <li>standardize (bool, optional) : Whether to standardise the data (Unit-variance scaling). Defaults to True.</li> <li> <p>max_tol (float, optional) : Maximum tolerance allowed when using the iterative NIPALS algorithm. Defaults to 1e-14.     nipals_convergence_norm (int, optional): Order of the norm that is used to calculate the difference of      the super-score vectors between subsequential iterations of the NIPALS algorithm.      Following orders are available:</p> ord norm for matrices norm for vectors None Frobenius norm 2-norm 'fro' Frobenius norm -- 'nuc' nuclear norm -- inf max(sum(abs(x), axis=1)) max(abs(x)) -inf min(sum(abs(x), axis=1)) min(abs(x)) 0 -- sum(x != 0) 1 max(sum(abs(x), axis=0)) as below -1 min(sum(abs(x), axis=0)) as below 2 2-norm (largest sing. value) as below -2 smallest singular value as below other -- sum(abs(x)ord)(1./ord) <p>Defaults to 2.</p> </li> <li> <p>calc_all (bool, optional) : Whether to calculate all internal attributes for the used method. Some methods do not need     to calculate all attributes (i.e., scores, weights) to obtain the regression coefficients used for prediction.     Setting this parameter to False will omit these calculations for efficiency and speed. Defaults to True.</p> </li> <li>sparse_data (bool, optional) : NIPALS is the only algorithm that can handle sparse data using the method of H. Martens     and Martens (2001) (p. 381). If this parameter is set to True, the method will be forced to NIPALS and sparse data     is allowed. Without setting this parameter to True, sparse data will not be accepted. Defaults to False.</li> <li>copy (bool, optional) : Whether the deflation should be done on a copy. Not using a copy might alter the input data     and have unforeseeable consequences. Defaults to True.</li> </ul> <p>Attributes</p> <ul> <li>n_components (int) : Number of Latent Variables (LV).</li> <li>Ts_ (array) : (X-Side)super scores [n, k].</li> <li>T_ (list) : (X-Side) block scores [i] [n, k]</li> <li>W_ (list) : (X-Side) block weights [n] [pi, k]</li> <li>A_ (array) : (X-Side) block importances/super weights [i, k]</li> <li>P_ (list, block) : (X-Side) loadings [i] [pi, k]</li> <li>R_ (array) : (X-Side) x_rotations R=W(PTW)-1</li> <li>explained_var_x_ (list) : (X-Side) explained variance in X per LV [k]</li> <li>explained_var_xblocks_ (array) : (X-Side) explained variance in each block Xi [i, k]</li> <li>beta_ (array) : (X-Side) regression vector \ud835\udefd [p, q]</li> <li>U_ (array) : (Y-Side) scoresInitialize [n, k]</li> <li>V_ (array) : (Y-Side) loadings [q, k]</li> <li>explained_var_y_ (list) : (Y-Side) explained variance in Y [k] A_corrected_ (array, (X-Side) normalized block importances Accor,ik=Aik\u22c5(1-pip)</li> </ul> <p>Methods:</p>"},{"location":"api_references/MamsiPls/#estimate_lv","title":".estimate_lv","text":"<p>source <pre><code>.estimate_lv(\n   x, y, n_components = 10, no_fold = 5, y_continuous = False, metric = 'auc',\n   plateau_threshold = 0.01, increase_threshold = 0.05, get_scores = False\n)\n</code></pre></p> <p>A method to estimate the number of latent variables (LVs)/components in the MB-PLS model.</p> <p>Args</p> <ul> <li>x (array or list[array]) : All blocks of predictors x1, x2, ..., xn. Rows are observations, columns are features/variables.</li> <li>y (array) : A 1-dim or 2-dim array of reference values, either continuous or categorical variable.</li> <li>n_components (int, optional) : Number of components/LVs. Defaults to 10.</li> <li>no_fold (int, optional) : Number of folds for k-fold cross-validation. Defaults to 5.</li> <li>y_continuous (bool, optional) : Whether the outcome is a continuous variable. Defaults to False.</li> <li>metric (str, optional) : Metric to use to estimate the number of LVs; available options: ['AUC', 'precision', 'recall', 'f1'] for      categorical outcome variables and ['q2'] for continuous outcome variable.      Defaults to 'AUC'.</li> <li>plateau_threshold (float, optional) : Maximum increase for a sequence of LVs to be considered a plateau. Must be non-negative. Defaults to 0.01.</li> <li>increase_threshold (float, optional) : Minimum increase to be considered a bend. Must be non-negative. Defaults to 0.05.</li> <li>get_scores (bool, optional) : Whether to return measured scores as a Pandas DataFrame. Defaults to False.</li> </ul> <p>Returns</p> <ul> <li>DataFrame  : Measured scores as a Pandas DataFrame.</li> </ul>"},{"location":"api_references/MamsiPls/#evaluate_class_model","title":".evaluate_class_model","text":"<p>source <pre><code>.evaluate_class_model(\n   x, y\n)\n</code></pre></p> <p>Evaluate classification MB-PLS model using a testing dataset.</p> <p>Args</p> <ul> <li>x (array or list[array]) : All blocks of predictors x1, x2, ..., xn. Rows are observations, columns are features/variables.</li> <li>y (array) : 1-dim or 2-dim array of reference values - categorical variable.</li> </ul> <p>Returns</p> <ul> <li>array  : Predicted y variable based on training set predictors.</li> </ul>"},{"location":"api_references/MamsiPls/#evaluate_regression_model","title":".evaluate_regression_model","text":"<p>source <pre><code>.evaluate_regression_model(\n   x, y\n)\n</code></pre></p> <p>Evaluate regression MB-PLS model using a testing dataset.</p> <p>Args</p> <ul> <li>x (array or list[array]) : All blocks of predictors x1, x2, ..., xn. Rows are observations, columns are features/variables.</li> <li>y (array) : 1-dim or 2-dim array of reference values - continuous variable.</li> </ul> <p>Returns</p> <ul> <li>array  : Predicted y variable based on training set predictors.</li> </ul>"},{"location":"api_references/MamsiPls/#mb_vip","title":".mb_vip","text":"<p>source <pre><code>.mb_vip(\n   plot = False\n)\n</code></pre></p> <p>Multi-block Variable Importance in Projection (MB-VIP) for multiblock PLS model.</p> <p>Adaptation of C. Wieder et al., (2024). PathIntegrate, doi: 10.1371/journal.pcbi.1011814.</p> <p>Args</p> <ul> <li>plot (bool, optional) : Whether to plot MB-VIP scores. Defaults to False.</li> </ul> <p>Returns</p> <ul> <li>array  : MB-VIP scores.</li> </ul>"},{"location":"api_references/MamsiPls/#mb_vip_permtest","title":".mb_vip_permtest","text":"<p>source <pre><code>.mb_vip_permtest(\n   x, y, n_permutations = 1000, return_scores = False\n)\n</code></pre></p> <p>Calculate empirical p-values for each feature by permuting the Y outcome variable <code>n_permutations</code> times and refitting the model. The p-values for each feature are calculated by counting the number of trials with MB-VIP greater than or equal to the observed test statistic, and dividing this by <code>n_permutations</code>.</p> <p>N.B. This method uses OpenMP to parallelise the code that relies on multi-threading exclusively. By default, the implementations using OpenMP will use as many threads as possible, i.e. as many threads as logical cores. This is available by default on systems with macOS and MS Windows. Running this method on a computer clusters / High Performance Computing (HPC) system, including Imperial HPC, requires additional Joblib parallelisation. A parallelised permutation test function can be found at function can be found in the  MAMSI Tutorials repository. If you are an Imperial colleague,  do not hesitate to contact me for support on how to set up the configuration PBS file for this job.</p> <p>Args</p> <ul> <li>x (array or list[array]) : All blocks of predictors x1, x2, ..., xn. Rows are observations, columns are features/variables.</li> <li>y (array) : 1-dim or 2-dim array of reference values, either continuous or categorical variable.</li> <li>n_permutations (int, optional) : Number of permutation tests. Defaults to 1000.</li> <li>return_scores (bool, optional) : Whether to return MB-VIP scores for each permuted null model. Defaults to False.</li> </ul> <p>Returns</p> <ul> <li>array  : Returns an array of p-values for each feature. If <code>return_scores</code> is True, then a matrix of MB-VIP scores for each permuted null model is returned as well.</li> </ul>"},{"location":"api_references/MamsiStructSearch/","title":"MamsiStructSearch","text":""},{"location":"api_references/MamsiStructSearch/#mamsistructsearch","title":"MamsiStructSearch","text":"<p>source <pre><code>MamsiStructSearch(\n   rt_win = 5, ppm = 15\n)\n</code></pre></p> <p>A class for performing structural search on multi-modal MS data using. The class allows to search for structural signatures in LC-MS data based on their m/z and RT. These structural signatures include isotopologues and adduct patterns.</p> <p>Attributes</p> <ul> <li>assay_links (list) : List of data frames containing links for each assay.</li> <li>intensities (numpy.ndarray) : Array of LC-MS intensity data.</li> <li>rt_win (int) : Retention time tolerance window.</li> <li>ppm (int) : Mass-to-charge ratio (m/z) tolerance in ppm.</li> <li>feature_metadata (pandas.DataFrame) : Data frame containing feature metadata extracted from column names.</li> <li>structural_links (pandas.DataFrame) : Data frame containing structural clusters.</li> </ul> <p>Args</p> <ul> <li>rt_win (int, optional) : Retention time tolerance window. Defaults to 5.</li> <li>ppm (int, optional) : Mass-to-charge ratio (m/z) tolerance in ppm. Defaults to 15.</li> </ul> <p>Methods:</p>"},{"location":"api_references/MamsiStructSearch/#load_msi","title":".load_msi","text":"<p>source <pre><code>.load_msi(\n   df\n)\n</code></pre></p> <p>Imports MSI intensity data and extracts feature metadata from column names.</p> <p>Args</p> <ul> <li>df (pandas.DataFrame) : Data frame with MSI intensity data.<ul> <li>rows: samples</li> <li>columns: features (m/z peaks)     Column names in the format:              For example:         149.111"},{"location":"api_references/MamsiStructSearch/#load_lcms","title":".load_lcms","text":"<p>source <pre><code>.load_lcms(\n   df\n)\n</code></pre></p> <p>Imports LC-MS intensity data and extracts feature metadata from column names.</p> <p>Args</p> <ul> <li>df (pandas.DataFrame) : Data frame with LC-MS intensity data.<ul> <li>rows: samples</li> <li>columns: features (LC-MS peaks)     Column names in the format:         m/z     For example:         HPOS_233.25_149.111m/z"},{"location":"api_references/MamsiStructSearch/#get_structural_clusters","title":".get_structural_clusters","text":"<p>source <pre><code>.get_structural_clusters(\n   adducts = 'all', annotate = True\n)\n</code></pre></p> <p>Searches structural signatures in LC-MS data based on their m/z and RT. These structural signatures include  isotopologues, adduct patterns and cross-assay links.</p> <p>Args</p> <ul> <li>adducts (str, optional) : Define what type of adducts to .      Possible values are:         - 'all': All adducts combinations (based on Fiehn Lab adduct calculator).         - 'most-common': Most common adducts for ESI (based on Waters adducts documentation).     Defaults to 'all'.</li> <li>annotate (bool, optional) : Annotate significant features based on National Phenome Centre RIO data.     Only to be run if the data was analysed by the National Phenome Centre or analysis followed their     conventions and protocols. For more information see https://doi.org/10.1021/acs.analchem.6b01481      or https://phenomecentre.org.     Uses semi-targeted annotations for selected compounds.     Defaults to True.</li> </ul> <p>Returns</p> <ul> <li>list (pandas.DataFrame) : DataFrame of significant features with structural clusters.</li> </ul>"},{"location":"api_references/MamsiStructSearch/#get_correlation_clusters","title":".get_correlation_clusters","text":"<p>source <pre><code>.get_correlation_clusters(\n   flat_method = 'constant', cut_threshold = 0.7, max_clusters = 5,\n   cor_method = 'pearson', linkage_method = 'complete', metric = 'euclidean',\n   **kwargs\n)\n</code></pre></p> <p>Clusters features based on their correlations. The method uses hierarchical clustering to create clusters. To flatten clusters, the method uses either a constant threshold or silhouette score.</p> <p>Args</p> <ul> <li>Flattens clusters based on a constant threshold (cut_threshold).<ul> <li>'silhouette': Flattens clusters based on most optimal silhouette score. Defaults to 'constant'.</li> </ul> </li> <li>cut_threshold (float, optional) : Constant threshold for flattening clusters. Defaults to 0.7.</li> <li>max_clusters (int, optional) : Maximum number of clusters for silhouette method. Defaults to 5.</li> <li>cor_method (str {'pearson', 'kendall', 'spearman'}, optional) : Method for calculation correlations. Defaults to 'pearson'.</li> <li>linkage_method (str, optional) : The linkage criterion determines which distance to use between sets of observation.     The algorithm will merge the pairs of cluster that minimise this criterion.<ul> <li>'single': Single linkage minimises the maximum distance between observations of pairs of clusters.</li> <li>'complete': Complete linkage minimises the maximum distance between observations of pairs of clusters.</li> <li>'average': Average linkage minimises the average of the distances between all observations of pairs of clusters.</li> <li>'ward': Ward minimises the variance of the clusters being merged.</li> <li>'weighted': Weighted linkage minimises the sum of the product of the distances and the number of observations in pairs of clusters.     Only available for 'constant' flatting method.</li> <li>'centroid': Centroid linkage minimises the distance between the centroids of clusters.     Only available for 'constant' flatting method.</li> <li>'median': Median linkage minimises the distance between the medians of clusters.     Only available for 'constant' flatting method. Defaults to 'complete'.</li> </ul> </li> <li>metric (str, optional) : The distance metric to use. The metric to use when calculating distance between instances in a feature array.     Metric used to compute the linkage. Can be \u201ceuclidean\u201d, \u201cl1\u201d, \u201cl2\u201d, \u201cmanhattan\u201d, \u201ccosine\u201d, or \u201cprecomputed\u201d.     If linkage is \u201cward\u201d, only \u201ceuclidean\u201d is accepted. If \u201cprecomputed\u201d, a distance matrix is needed as input for the fit method.     Defaults to 'euclidean'. flat_method (str {'constant', 'silhouette'}, optional):     Method for cluster flattening:</li> </ul>"},{"location":"api_references/MamsiStructSearch/#get_structural_network","title":".get_structural_network","text":"<p>source <pre><code>.get_structural_network(\n   include_all = False, interactive = False, return_nx_object = False,\n   output_file = 'interactive.html', labels = False, master_file = None\n)\n</code></pre></p> <p>Generates a structural network graph based on the provided master file or the loaded structural links data. The method creates a network graph based on the generated structural data or the provided master file. The network graph includes nodes representing features and edges representing different types of links. The graph can be displayed interactively using pyvis.network or using NetworkX and matplotlib. The graph can be saved as a NetworkX object if return_nx_object is True.</p> <p>Args</p> <ul> <li>include_all (bool, optional) : Whether to include all features in the network, even if they are not structurally linked to other features.     Defaults to False.</li> <li>interactive (bool, optional) : Whether to display the network graph interactively using pyvis.network.     If False, the network graph is displayed using NetworkX and Matplotlib.     Defaults to False.</li> <li>return_nx_object (bool, optional) : Whether to return the NetworkX object representing the network graph edited in CytoScape.      Defaults to False.</li> <li>output_file (str, optional) : The name of the output file when displaying the network graph interactively using pyvis.network.     Only applicable when interactive is True.      Defaults to 'interactive.html'.</li> <li>labels (bool, optional) : Whether to display labels for the nodes in the network graph.     Only applicable when interactive is False.      Defaults to False.</li> <li>master_file (pd.DataFrame, optional) : The master file containing necessary columns for generating the network.     This is intended for cases when structural links required manual curation (e.g. manually assigned isotopologue groups, adduct groups, etc.)     If not provided, the function uses the loaded structural links data.     Required columns:          - Feature: Feature ID (e.g. HPOS_233.25_149.111m/z)         - Assay: Assay name (e.g. HPOS)         - Isotopologue group (groups features with similar isotopologue patterns)         - Isotopologue pattern (e.g. 0, 1, 2 ... N representing M+0, M+1, M+2 ... M+N)         - Adduct group (groups features with similar adduct patterns)         - Adduct (adduct label, e.g. [M+H]+, [M-H]-)         - Structural cluster (groups features with similar isotopologue and adduct patterns)         - Correlation cluster (flattened hierarchical cluster from get_correlation_clusters()         - Cross-assay link (links features across different assays)         - cpdName (compound name, optional)     Defaults to None.</li> </ul> <p>Returns</p> <ul> <li>None  : The NetworkX object representing the network graph, if return_nx_object is True.     Edge weights represent the type of link between features:         - Isotopologue: 1         - Adduct: 5         - Cross-assay link: 10     Otherwise, None.</li> </ul> <p>Raises</p> <ul> <li>RuntimeWarning  : If no data is loaded and no master file is provided.</li> <li>RuntimeWarning  : If the provided master file is missing necessary columns.</li> </ul>"}]}
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MAMSI","text":"<p>MAMSI is a Python framework designed for the integration of multi-assay mass spectrometry datasets.  In addition, the MAMSI framework provides a platform for linking statistically significant features of untargeted multi-assay liquid chromatography \u2013 mass spectrometry (LC-MS) metabolomics datasets into clusters defined by their structural properties based on mass-to-charge ratio (m/z) and retention time (RT).</p> <p>N.B. the framework was tested on metabolomics phenotyping data, but it should be usable with other types of LC-MS data.</p>"},{"location":"#overview","title":"Overview","text":""},{"location":"#features","title":"Features","text":"<ul> <li>Data integration analysis using the Multi-Block Partial Least Squares (MB-PLS) [1] algorithm. The <code>MamsiPls</code> class inherits from the <code>mbpls</code> package [1]. For more information on MB-PLS, please visit mbpls Documentation.</li> <li>Multi-Block Variable Importance in Projection (MB-VIP) [2].</li> <li>Estimation of statistically significant features (variables) using MB-VIP and permutation testing.</li> <li>Linking significant features into clusters defined by structural properties of metabolites.</li> <li>Feature network links.</li> <li>Annotation of untargeted LC-MS features (only supported for assays analysed by the National Phenome Centre).</li> </ul>"},{"location":"#sources-and-materials","title":"Sources and Materials","text":"<p>The package source code is accessible via GitHub at https://github.com/kopeckylukas/py-mamsi</p> <p>Training materials including sample data can be found at https://github.com/kopeckylukas/py-mamsi-tutorials.</p>"},{"location":"#installation","title":"Installation","text":""},{"location":"#dependencies","title":"Dependencies","text":"<ul> <li>mbpls==1.0.4</li> <li>pandas</li> <li>numpy</li> <li>matplotlib</li> <li>scipy</li> <li>scikit-learn</li> <li>seaborn</li> <li>networkx</li> <li>pyvis</li> <li>joblib</li> </ul>"},{"location":"#user-installation","title":"User Installation","text":""},{"location":"#installing-with-pip","title":"Installing with Pip","text":"<p>You can install MAMSI from PyPI using pip:  <pre><code>pip install mamsi\n</code></pre></p>"},{"location":"#installing-from-source-code","title":"Installing from source-code","text":"<p>You can install MAMSI from source code given you have installed both Python (&gt;=3.9) and PIP.</p> <p>First, clone the repository from GitHub to your computer. You can use following commands if you have a version of Git installed on your computer:</p> <pre><code>git clone https://github.com/kopeckylukas/py-mamsi\n\ncd py-mamsi\n</code></pre> <p>When you are in the cloned project folder, type the following code to install MAMSI and all dependencies:  <pre><code>pip install .\n</code></pre></p> <p>Alternatively, you can install dependencies using pip and MAMSI using Python: <pre><code>pip install -r requirements.txt\npython setup.py develop\n</code></pre></p>"},{"location":"#issues-and-collaboration","title":"Issues and Collaboration","text":"<p>Thank you for supporting the MAMSI project. MAMSI is an open-source software and welcomes any form of contribution and support.</p>"},{"location":"#issues","title":"Issues","text":"<p>Please submit any bugs or issues via the project's GitHub issue page and any include details about the (<code>mamsi.__version__</code>) together with any relevant input data/metadata. </p>"},{"location":"#collaboration","title":"Collaboration","text":""},{"location":"#pull-requests","title":"Pull requests","text":"<p>You can actively collaborate on MAMSI package by submitting any changes via a pull request. All pull requests will be reviewed by the MAMSI team and merged in due course. </p>"},{"location":"#contributions","title":"Contributions","text":"<p>If you would like to become a contributor on the MAMSI project, please contact Lukas Kopecky.</p>"},{"location":"#acknowledgement","title":"Acknowledgement","text":"<p>This package was developed as part of Lukas Kopecky's PhD project at Imperial College London, funded by Waters UK. It is free to use, published under BSD 3-Clause licence.</p> <p>The authors of this package would like to acknowledge the authors of the mbpls package [1] which became the backbone of MAMSI. For more information on MB-PLS, please visit MB-PLS Documentation.</p> <p>Further, we would like to thank Prof Simon Lovestone and Dr Shivani Misra for allowing us to use their data, AddNeuroMed [3] and MY Diabetes [5] respectively, for the development of this package. </p>"},{"location":"#citing-us","title":"Citing us","text":"<p>If you use MAMSI in a scientific publication, we would appreciate citations. </p>"},{"location":"#release","title":"Release","text":"<pre><code>@misc{MAMSI2024,\n  author       = {Lukas Kopecky, Elizabeth J Want, Timothy MD Ebbels},\n  title        = {MAMSI: Multi-Assay Mass Spectrometry Integration},\n  year         = 2024,\n  url          = {https://doi.org/10.5281/zenodo.13619607},\n  note         = {Zenodo. Version 1.0.0},\n  doi          = {10.5281/zenodo.13619607}\n}\n</code></pre>"},{"location":"#publication","title":"Publication","text":"<p>The MAMSI publication is currently under the review process. </p>"},{"location":"#references","title":"References","text":"<p>[1] A. Baum and L. Vermue, \"Multiblock PLS: Block dependent prediction modeling for Python,\" J. Open Source Softw., vol. 4, no. 34, 2019, doi: 10.21105/joss.01190.</p> <p>[2] C. Wieder et al., \"PathIntegrate: Multivariate modelling approaches for pathway-based multi-omics data integration,\" PLOS Comput. Biol., vol. 20, no. 3, p. e1011814, Mar 2024, doi: 10.1371/journal.pcbi.1011814.</p> <p>[3] S. Lovestone et al., \"AddNeuroMed\u2014The European Collaboration for the Discovery of Novel Biomarkers for Alzheimer's Disease,\" Ann. N. Y. Acad. Sci, vol. 1180, no. 1, pp. 36-46, 2009, doi: 10.1111/j.1749-6632.2009.05064.x. [4] A. M. Wolfer et al., \"peakPantheR, an R package for large-scale targeted extraction and integration of annotated metabolic features in LC\u2013MS profiling datasets,\" Bioinformatics, vol. 37, no. 24, pp. 4886-4888, 2021, doi: 10.1093/bioinformatics/btab433.</p> <p>[5] S. Misra et al., \"Systematic screening for monogenic diabetes in people of South Asian and African Caribbean ethnicity: Preliminary results from the My Diabetes study,\" presented at the Diabet. Med., Mar 2018.</p> <p>[6]  M. Lewis et al., \u201cAn Open Platform for Large Scale LC-MS-Based Metabolomics ,\u201d ChemRxiv, 2022. doi: 10.26434/chemrxiv-2022-nq9k0.</p> <p>[7] C. A. Smith et al., \"XCMS:\u2009 Processing Mass Spectrometry Data for Metabolite Profiling Using Nonlinear Peak Alignment, Matching, and Identification,\" Anal. Chem., vol. 78, no. 3, pp. 779-787, Feb 2006, doi: 10.1021/ac051437y.</p> <p>[8] C. J. Sands et al., \"The nPYc-Toolbox, a Python module for the pre-processing, quality-control and analysis of metabolic profiling datasets,\" Bioinformatics, vol. 35, no. 24, pp. 5359-5360, 2019, doi: 10.1093/bioinformatics/btz566.</p>"},{"location":"#version-history","title":"Version History","text":""},{"location":"#v104","title":"v1.0.4","text":"<p>New Features</p> <ul> <li>Parallelised <code>.kfold_cv()</code></li> <li>Parallelised <code>.montecarlo_cv()</code></li> <li>Parallelised <code>.estimate_lv()</code></li> <li>Parallelised <code>.mb_vip_permtest()</code></li> </ul>"},{"location":"#v103","title":"v1.0.3","text":"<p>New Features </p> <ul> <li>k-fold cross-validation implemented as a method <code>.kfold_cv()</code> that can be used for model performance evaluation. This method includes GroupKFold option.</li> <li>Monte Carlo cross-validaton (MCCV), also nown as 'random sampling cross-validation' implemented as a method <code>.montecarlo_cv()</code> that can be used for model performance evaluation.</li> <li><code>.estimate_lv()</code> method now allows to choose between k-fold CV and MC-CV using parameter <code>method</code></li> </ul> <p>Bug Fixes and Behavioural Changes - Plot title for <code>.block_importance()</code> fixed. - For regression analysis, MSE metric changed to RMSE - For <code>.estimate_lv()</code> method, parameter <code>y_continuous=False</code> was replaced with <code>classification=True</code> </p>"},{"location":"#v102","title":"v1.0.2","text":"<p>New Features</p> <ul> <li>New method 'MamsiPls.block_importance()': Calculate the block importance for each block in the multiblock PLS model and plot the results.</li> </ul> <p>Minor Bug Fixes and Behavioural Changes</p> <ul> <li>Behavioural changes for <code>MamsiPls.mb_vip()</code>: The MB-VIP plot is now rendered by default, scores are not returned by default. New default arguments (plot=True, get_scores=False).</li> <li>Argument changes for <code>MamsiPls.estimate_lv()</code>: Old Arguments (no_folds, n_components) changed to (n_slplits, max_components) respectively. </li> <li>Plots: 'Verdana' is no longer the default font. The default font changed to Matplotlib default 'DejaVu Sans'.</li> <li>Updates to <code>MamsiStructSearch</code> class to comply with future warnings - Pandas 3.0.</li> </ul>"},{"location":"#v101","title":"v1.0.1","text":"<p>Minor Bugs Fixes </p> <ul> <li>Fixes instances where flattened correlation clusters were misaligned to structural clusters.</li> <li>Readme licence badge links directly to GitHub licence file (URL).</li> </ul>"},{"location":"#v100","title":"v1.0.0","text":"<p>Initial Release</p>"},{"location":"tutorials/","title":"Tutorials and Training Materials","text":""},{"location":"tutorials/#training-materials","title":"Training Materials","text":"<p>You can find all MAMSI training materials by visiting our MAMSI Tutorials repository. </p> <p>The <code>MamsiPls</code> class inherits from the <code>mbpls</code> package [1]. For more information on MB-PLS, please visit mbpls Documentation.</p>"},{"location":"tutorials/#tutorial-example-classification","title":"Tutorial Example - Classification","text":"<p>The Multi-Assay Mass Spectrometry Integration (MAMSI) workflow allows for integrative analysis of multiple metabolomics LC-MS assays data. The MAMSI workflow utilises a multi-block partial-least squares (MB-PLS) discriminant analysis algorithm, which allows for the integration of multiple assays and the subsequent identification of the most significant predictors (features). The identification of statistically significant predictors is done using a multi-block version of the variable importance in projection (MB-VIP) procedure coupled with permutation testing. This enables us to obtain empirical p-values for each feature across all assays. MAMSI also offers an easy interpretation of significant features. This is done by grouping of the significant features based on their structural properties (mass-to-charge ratio and retention time) and compared to their correlations. This can be visualised by a network plot.</p> <p>This notebook showcases the use of the MAMSI workflow for the prediction (classification) of the biological sex of patients within the AddNeuroMed cohort [3] - dataset of Alzheimer's disease patients. For this task, we will use 3 metabolomics blood serum assays. The assays were processed by the National Phenome Centre following the NPC protocol [6]. Subsequently, data were pre-processed using XCMS [7] and nPYc toolbox [8].</p> <p>Assays Overview </p> Assay Number of features Description HPOS 681 Hydrophilic interaction liquid chromatography (HILIC) positive ionisation LPOS 4,886 Lipidomic reversed  phase chromatography positive ionisation LNEG 2,091 Lipidomic reversed phase chromatography negative ionisation <p>Outcome variable: Biological Sex</p> Class Number of samples Male 283 Female 294"},{"location":"tutorials/#load-packages","title":"Load Packages","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom mamsi.mamsi_pls import MamsiPls\nfrom mamsi.mamsi_struct_search import MamsiStructSearch\nfrom matplotlib import pyplot as plt\n</code></pre>"},{"location":"tutorials/#load-sample-data","title":"Load Sample Data","text":"<p> Data used within this quickstart guide originate from the the AddNeuroMed [1] cohort - dataset of Alzheimer's disease patients.  You can download the sample data from this link.</p> <pre><code>metadata = pd.read_csv('../sample_data/alz_metadata.csv')\n# The PLS algorithm requires the response variable to be numeric. \n# We will encode the outcome \"Gender\" (Biological Sex) as 1 for female and 0 for male subjects. \ny = metadata[\"Gender\"].apply(lambda x: 1 if x == 'Female' else 0)\n\n# Import LC-MS data\n# Add prefix to the columns names. This will be crucial for interpreting the results later on.\nhpos = pd.read_csv('./sample_data/alz_hpos.csv').add_prefix('HPOS_')\nlpos = pd.read_csv('./sample_data/alz_lpos.csv').add_prefix('LPOS_')\nlneg = pd.read_csv('./sample_data/alz_lneg.csv').add_prefix('LNEG_')\n</code></pre>"},{"location":"tutorials/#split-data","title":"Split data","text":"<p>Split the dataset into training and testing subsets. The training subset will be used to fit the model, for cross-validation and for estimation of the number of latent variables. Then, the testing subset will be used as an independent dataset to assess the performance of the model. This will ensure that the model is not over-fitted to the data and that it can predict the outcome of the samples. </p> <pre><code># Split data in the ratio 90:10 for training and testing respectively.\nhpos_train, hpos_test, y_train, y_test = train_test_split(hpos, y, test_size=0.1, random_state=42)\n\n#\u00a0Split the other two blocks based on the indices of the hpos block.\nlpos_train = lpos.iloc[hpos_train.index,:]\nlneg_train = lneg.iloc[hpos_train.index,:]\n\nlpos_test = lpos.iloc[hpos_test.index,:]\nlneg_test = lneg.iloc[hpos_test.index,:]\n</code></pre>"},{"location":"tutorials/#fit-model","title":"Fit Model","text":""},{"location":"tutorials/#fit-mb-pls-model-and-estimate-lvs","title":"Fit MB-PLS Model and Estimate LVs","text":"<p>As an example, we will start by fitting a MB-PLS model (from the MamsiPls class) with 1 component/latent variable (LV) and using the standard scaler. As a result, we will obtain super scores, block loadings and scores and block importances. Note that MamsiPls inherits its behaviour from the mbpls [1] model which, by default, uses the NIPALS algorithm. To see all possible configurations for the MamsiPls and mbpls models, see the mbpls documentation.</p> <p>We can then estimate the number of latent variables in the model. The MamsiPls class provides method <code>MamsiPls.estimate_lv()</code>\u00a0to estimate number of LVs in the model using k-fold cross-validation (CV). The k-fold CV is repeated k-times corresponding to number of LVs in the most complex model. The lowest possible number of LVs where the model stabilised (model performance did not rise by adding more LVs) was selected as the final model.</p> <p>For the classification task, you can choose from <code>'auc', 'precision', 'recall', 'f1', 'accuracy'</code> metrics to perform the LV estimation on the mean value of validation CV splits. <pre><code>mamsipls = MamsiPls(n_components=1)\nmamsipls.fit([hpos_train, lpos_train, lneg_train], y_train)\n\n# Estimate the number of latent variables in you model\nmamsipls.estimate_lv([hpos_train, lpos_train, lneg_train], y_train, metric='auc')\n</code></pre> The LV estimation result shows that the model has 6 latent variables/components. Adding more LVs to the model could lead to overfitting. </p>"},{"location":"tutorials/#evaluate-final-model","title":"Evaluate Final Model","text":"<p>We can evaluate the performance of the model by predicting the outcome on an independent (testing) dataset that has not been used for model training. For this, we will use the 'testing' subset that we obtained during the train-test-split. </p> <p>We can get the performance scores by calling the <code>mamsipls.evaluate_class_model()</code> method.</p> <p><pre><code>predicted = mamsipls.evaluate_class_model([hpos_test, lpos_test, lneg_test], y_test.array)\n</code></pre> </p> Metric Score Accuracy 0.966 Recall 1.0 Specificity 0.943 F<sub>1</sub> Score 0.971 AUC 0.933 <p>The scores and confusion matrix above indicate that the model performance has improved. If we are happy with such model, we can start with model interpretation. </p>"},{"location":"tutorials/#estimate-feature-importance","title":"Estimate Feature Importance","text":"<p>We can start with reviewing the Multi-Block Variable Importance in Projection (MB-VIP) scores [2]. The MB-VIP metric is the sum (weighted by the amount of variance of Y explained by each respective component) of the squared weight values. It provides a summary of the importance of a variable accounting for all weight vectors. VIPs are bounded between 0 (no effect) and infinity. Because it is calculated from the weights w, for PLS models with a single component, these are directly proportional to w<sup>2</sup>. The VIP metric has the disadvantage of pooling together w vectors from components which contribute a very small magnitude to the model's R<sup>2</sup>Y.</p>"},{"location":"tutorials/#multiblock-variable-importance-in-projection","title":"Multiblock Variable Importance in Projection","text":"<p><pre><code>mb_vip = mamsipls.mb_vip(plot=True)\n</code></pre> </p> <p>Unfortunately, the assessment of variable importance in MB-PLS multivariate models is not straightforward, given the choice of parameters and their different interpretations, especially in models with more than 1 LV. To obtain a ranking of variables from the data matrix X associated with Y, we recommend using permutation testing coupled with the MB-VIP metric to estimate the empirical p-values for each variable.</p>"},{"location":"tutorials/#permutation-testing","title":"Permutation Testing","text":"<p>You can perform permutation testing using the <code>MamsiPls.mb_vip_permtest()</code> method. We recommend to perform at least 10,000 permutations, but ideally &gt;100,000 for a good p-value estimate. You can find pre-calculated p-values at link.</p> <p>We can review the empirical null distribution of the MB-VIP scores of a statistically non-significant feature (1) and compare it to a statically significant feature (5769).</p> <p>In both cases, the dashed line indicates the MB-VIP score for each feature of the observed (non-permuted) model. </p> <p>The empirical p-values for the i <sup>th</sup> feature are calculated by dividing the number of MB-VIP scores of the null (permuted) models for the i <sup>th</sup> feature higher than the observed MB-VIP score for i <sup>th</sup> feature, by the total number of permutations.</p> <p><pre><code>p_vals, null_vip = mamsipls.mb_vip_permtest([hpos, lpos, lneg], y, n_permutations=10000, return_scores=True)\n</code></pre> </p> <p>Note that the pre-calculated <code>null_vip</code> file contains MB-VIP scores for first 400 null models (permutations) only so the p-value displayed on the plot below does not correspond with the plot itself.</p>"},{"location":"tutorials/#interpret-statistically-significant-features","title":"Interpret Statistically Significant Features","text":"<p><pre><code># merge the LC-MS data into a single data frame\nx = pd.concat([hpos, lpos, lneg], axis=1)\n\n# Select features with p-value &lt; 0.01.\n# You can also apply multiple testing correction methods to adjust the p-value threshold.\nmask = np.where(p_vals &lt; 0.01)\nselected = x.iloc[:, mask[0]]\n</code></pre> You can use MAMSI Structural Search tool (<code>MamsiStructSearch()</code>) to help you understand the nature of statistically significant features. </p> <p>Firstly, all features are split into retention time (RT) windows of 5 seconds intervals, then each RT window is searched for isotopologue signatures by searching mass differences of 1.00335 Da between mass-to-charge ratios (m/z) of the features; if two or more features resemble a mass isotopologue signature then they are grouped together. This is followed by a search for common adduct signatures. This is achieved by calculating hypothetical neutral masses based on common adducts in electrospray ionisation. If hypothetical neutral masses match for two or more features within a pre-defined tolerance (15 ppm) then these features are grouped together. Overlapping adduct clusters and isotopologue clusters are then merged to form structural clusters. Further, we search cross-assay clusters using [M+H]<sup>+</sup>/[M-H]<sup>-</sup> as link references. Additionally, our structural search tool, that utilises region of interest (ROI) files from peakPantheR [4], allows for automated annotation of  some features based on the RT for a given chromatography and m/z.</p>"},{"location":"tutorials/#mamsi-structural-search-tool","title":"MAMSI Structural Search Tool","text":"<p><pre><code># First, we need to define the MamsiStructSearch object \n# and choose the tolerances for the retention time and m/z matching.\nstruct = MamsiStructSearch(rt_win=5, ppm=10)\n\n# Load Selected LC-MS Features\nstruct.load_lcms(selected)\n\nstruct.get_structural_clusters(annotate=True)\npd.set_option(\"display.max_rows\", 15)\n</code></pre> We can then perform the structural search.  Note, use the <code>annotate=True</code> to get the annotation for selected features  only if your LC-MS data originate from the National Phenome Centre or follow the NPC protocol.</p> <p>We also need to perform hierarchical correlation clustering between selected features.  You can choose either the silhouette method or define a straight cut-off on the dendrogram <pre><code>struct.get_correlation_clusters(flat_method='silhouette', max_clusters=11)\n</code></pre> Best number of clusters based on silhouette score: 8  Silhouette score for 8 clusters: 0.2436798413177305</p> <p> </p> <p>Finally we can visualise the structural relationships using a network plot.  The different node colours represent different flattened hierarchical correlation clusters, while the edges between nodes identify their structural links. You can also save the network as an NX object and review in Cytoscape to get better insight on what the structural relationship between individual features are (e.g. adduct links, isotopologues, cross-assay links). <pre><code>network = struct.get_structural_network(include_all=True, interactive=False, labels=True, return_nx_object=True)\n</code></pre> </p>"},{"location":"api_references/","title":"MAMSI","text":"<p>MAMSI is a Python framework designed for the integration of multi-assay mass spectrometry datasets.  In addition, the MAMSI framework provides a platform for linking statistically significant features of untargeted multi-assay liquid chromatography \u2013 mass spectrometry (LC-MS) metabolomics datasets into clusters defined by their structural properties based on mass-to-charge ratio (m/z) and retention time (RT).</p> <p>N.B. the framework was tested on metabolomics phenotyping data, but it should be usable with other types of LC-MS data.</p>"},{"location":"api_references/#features","title":"Features","text":"<ul> <li>Data integration analysis using the Multi-Block Partial Least Squares (MB-PLS) [1] algorithm.</li> <li>Multi-Block Variable Importance in Projection (MB-VIP) [2].</li> <li>Estimation of statistically significant features (variables) using MB-VIP and permutation testing.</li> <li>Linking significant features into clusters defined by structural properties of metabolites.</li> <li>Feature network links.</li> <li>Annotation of untargeted LC-MS features (only supported for assays analysed by the National Phenome Centre).</li> </ul>"},{"location":"api_references/#documentation","title":"Documentation","text":"<p>The documentation for this package is available at https://kopeckylukas.github.io/py-mamsi/.</p>"},{"location":"api_references/#installation","title":"Installation","text":""},{"location":"api_references/#dependencies","title":"Dependencies","text":"<ul> <li>mbpls==1.0.4</li> <li>pandas</li> <li>numpy</li> <li>matplotlib</li> <li>scipy</li> <li>scikit-learn</li> <li>seaborn</li> <li>networkx</li> <li>pyvis</li> <li>joblib</li> </ul>"},{"location":"api_references/#user-installation","title":"User Installation","text":""},{"location":"api_references/#installing-with-pip","title":"Installing with Pip","text":"<p>You can install MAMSI from PyPI using pip:  <pre><code>pip install mamsi\n</code></pre></p>"},{"location":"api_references/#installing-from-source-code","title":"Installing from source-code","text":"<p>You can install it directly from source code given you have installed both Python (&gt;=3.9) and PIP.</p> <pre><code>git clone https://github.com/kopeckylukas/py-mamsi\ncd py-mamsi\n</code></pre> <p>When you are in the cloned project folder, type the following code to install MAMSI and all dependencies:  <pre><code>pip install .\n</code></pre></p> <p>Alternatively, you can install dependencies using pip and MAMSI using Python: <pre><code>pip install -r requirements.txt\npython setup.py develop\n</code></pre></p>"},{"location":"api_references/#quickstart","title":"Quickstart","text":"<p>You can find all MAMSI tutorials by visiting our MAMSI Tutorials repository. To import and instantiate the package objects, you can follow this quickstart guide:</p> Read more  **Load Packages** <pre><code>from mamsi.mamsi_pls import MamsiPls\nfrom mamsi.mamsi_struct_search import MamsiStructSearch\nimport pandas as pd\nimport numpy as np\n</code></pre>  **Load Sample Data and MamsiPls Model**   Data used within this quickstart guide originate from the AddNeuroMed cohort [[3](#references)] - dataset of Alzheimer's disease patients.  You can download the sample data from this [link](https://github.com/kopeckylukas/py-mamsi-tutorials/tree/main/sample_data).   <pre><code>metadata = pd.read_csv('./sample_data/alz_metadata.csv')\n# The PLS algorithm requires the response variable to be numeric. \n# We will encode the outcome \"Gender\" (Biological Sex) as 1 for female and 0 for male subjects. \ny = metadata[\"Gender\"].apply(lambda x: 1 if x == 'Female' else 0)\n\n# Import LC-MS data\n# Add prefix to the columns names. This will be crucial for interpreting the results later on.\nhpos = pd.read_csv('./sample_data/alz_hpos.csv').add_prefix('HPOS_')\nlpos = pd.read_csv('./sample_data/alz_lpos.csv').add_prefix('LPOS_')\nlneg = pd.read_csv('./sample_data/alz_lneg.csv').add_prefix('LNEG_')\n</code></pre>  Fit MamsiPls Model and Estimate LVs <pre><code>mamsipls = MamsiPls(n_components=1)\nmamsipls.fit([hpos, lpos, lneg], y)\n</code></pre>  **Estimate Latent Variables and Feature Importance** <pre><code>mamsipls.estimate_lv([hpos, lpos, lneg], y, metric='auc')\n</code></pre>  You can visualise the MB-VIP: <pre><code>mb_vip = mamsipls.mb_vip(plot=True)\n</code></pre> or estimate empirical p-values for all features:   <pre><code>p_vals, null_vip = mamsipls.mb_vip_permtest([hpos, lpos, lneg], y, n_permutations=10000, return_scores=True)\n</code></pre>  **Interpret Statistically Significant Features** <pre><code>x = pd.concat([hpos, lpos, lneg], axis=1)\n\nmask = np.where(p_vals &lt; 0.01)\nselected = x.iloc[:, mask[0]]\n</code></pre> Use `MamsiStrustSearch` to search for structural links within the statistically significant features.  Firstly, all features are split into retention time (*RT*) windows of 5 seconds intervals, then each RT window is searched for isotopologue signatures by searching mass differences of 1.00335 Da between mass-to-charge ratios (*m/z*) of the features; if two or more features resemble a mass isotopologue signature then they are grouped together. This is followed by a search for common adduct signatures. This is achieved by calculating hypothetical neutral masses based on common adducts in electrospray ionisation. If hypothetical neutral masses match for two or more features within a pre-defined tolerance (15 *ppm*) then these features are grouped together. Overlapping adduct clusters and isotopologue clusters are then merged to form structural clusters. Further, we search cross-assay clusters using [M+H]<sup>+</sup>/[M-H]<sup>-</sup> as link references. Additionally, our structural search tool, that utilises region of interest [(ROI) files](https://github.com/phenomecentre/npc-open-lcms) from peakPantheR [[4](#references)], allows for automated annotation of  some features based on the *RT* for a given chromatography and *m/z*.  <pre><code>struct = MamsiStructSearch(rt_win=5, ppm=10)\nstruct.load_lcms(selected)\nstruct.get_structural_clusters(annotate=True)\n</code></pre> Further, you can use the `MamsiStrustSearch.get_correlation_clusters()` method to find correlation clusters. <pre><code>struct.get_correlation_clusters(flat_method='silhouette', max_clusters=11)\n</code></pre> Finally, we visualise the structural relationships using a network plot. The different node colours represent different flattened hierarchical correlation clusters, while the edges between nodes identify their structural links. You can also save the network as an NX object and review in Cytoscape to get better insight on what the structural relationships between individual features are (e.g. adduct links, isotopologues, cross-assay links). <pre><code>network = struct.get_structural_network(include_all=True, interactive=False, labels=True, return_nx_object=True)\n</code></pre> <p></p>"},{"location":"api_references/#issues-and-collaboration","title":"Issues and Collaboration","text":"<p>Thank you for supporting the MAMSI project. MAMSI is an open-source software and welcomes any form of contribution and support.</p>"},{"location":"api_references/#issues","title":"Issues","text":"<p>Please submit any bugs or issues via the project's GitHub issue page and any include details about the (<code>mamsi.__version__</code>) together with any relevant input data/metadata. </p>"},{"location":"api_references/#collaboration","title":"Collaboration","text":""},{"location":"api_references/#pull-requests","title":"Pull requests","text":"<p>You can actively collaborate on MAMSI package by submitting any changes via a pull request. All pull requests will be reviewed by the MAMSI team and merged in due course. </p>"},{"location":"api_references/#contributions","title":"Contributions","text":"<p>If you would like to become a contributor on the MAMSI project, please contact Lukas Kopecky.</p>"},{"location":"api_references/#acknowledgement","title":"Acknowledgement","text":"<p>This package was developed as part of Lukas Kopecky's PhD project at Imperial College London, funded by Waters UK. It is free to use, published under BSD 3-Clause licence.</p> <p>The authors of this package would like to acknowledge the authors of the mbpls package [1] which became the backbone of MAMSI. For more information on MB-PLS, please visit MB-PLS Documentation.</p> <p>Further, we would like to thank Prof Simon Lovestone and Dr Shivani Misra for allowing us to use their data, AddNeuroMed [3] and MY Diabetes [5] respectively, for the development of this package. </p>"},{"location":"api_references/#citing-us","title":"Citing us","text":"<p>If you use MAMSI in a scientific publication, we would appreciate citations. </p>"},{"location":"api_references/#release","title":"Release","text":"<pre><code>@misc{MAMSI2024,\n  author       = {Lukas Kopecky, Elizabeth J Want, Timothy MD Ebbels},\n  title        = {MAMSI: Multi-Assay Mass Spectrometry Integration},\n  year         = 2024,\n  url          = {https://doi.org/10.5281/zenodo.13619607},\n  note         = {Zenodo. Version 1.0.0},\n  doi          = {10.5281/zenodo.13619607}\n}\n</code></pre>"},{"location":"api_references/#publication","title":"Publication","text":"<p>The MAMSI publication is currently under the review process. </p>"},{"location":"api_references/#references","title":"References","text":"<p>[1] A. Baum and L. Vermue, \"Multiblock PLS: Block dependent prediction modeling for Python,\" J. Open Source Softw., vol. 4, no. 34, 2019, doi: 10.21105/joss.01190.</p> <p>[2] C. Wieder et al., \"PathIntegrate: Multivariate modelling approaches for pathway-based multi-omics data integration,\" PLOS Comput. Biol., vol. 20, no. 3, p. e1011814, Mar 2024, doi: 10.1371/journal.pcbi.1011814.</p> <p>[3] S. Lovestone et al., \"AddNeuroMed\u2014The European Collaboration for the Discovery of Novel Biomarkers for Alzheimer's Disease,\" Ann. N. Y. Acad. Sci, vol. 1180, no. 1, pp. 36-46, 2009, doi: 10.1111/j.1749-6632.2009.05064.x.</p> <p>[4] A. M. Wolfer et al., \"peakPantheR, an R package for large-scale targeted extraction and integration of annotated metabolic features in LC\u2013MS profiling datasets,\" Bioinformatics, vol. 37, no. 24, pp. 4886-4888, 2021, doi: 10.1093/bioinformatics/btab433.</p> <p>[5] S. Misra et al., \"Systematic screening for monogenic diabetes in people of South Asian and African Caribbean ethnicity: Preliminary results from the My Diabetes study,\" presented at the Diabet. Med., Mar 2018.</p>"},{"location":"api_references/#version-history","title":"Version History","text":"Read more  ## v1.0.4 **New Features** - Parallelised `.kfold_cv()` - Parallelised `.montecarlo_cv()` - Parallelised `.estimate_lv()` - Parallelised `.mb_vip_permtest()`  ## v1.0.3 **New Features**  - *k*-fold cross-validation implemented as a method `.kfold_cv()` that can be used for model performance evaluation. This method includes GroupKFold option. - Monte Carlo cross-validaton (MCCV), also nown as 'random sampling cross-validation' implemented as a method `.montecarlo_cv()` that can be used for model performance evaluation. - `.estimate_lv()` method now allows to choose between *k*-fold CV and MC-CV using parameter `method`  **Bug Fixes and Behavioural Changes** - Plot title for `.block_importance()` fixed. - For regression analysis, MSE metric changed to RMSE - For `.estimate_lv()` method, parameter `y_continuous=False` was replaced with `classification=True`    ## v1.0.2 **New Features** - New method 'MamsiPls.block_importance()': Calculate the block importance for each block in the multiblock PLS model and plot the results.  **Minor Bug Fixes and Behaviour Changes** - Behavioural changes for `MamsiPls.mb_vip()`: The MB-VIP plot is now printed by default, scores are not returned by default. New default arguments (plot=True, get_scores=False). - Argument changes for `MamsiPls.estimate_lv()`: Old Arguments (no_folds, n_components) changed to (n_slplits, max_components) respectively.  - Plots: 'Verdana' is no longer the default font. The default font changed to Matplotlib default 'DejaVu Sans'. - Updates to `MamsiStructSearch` class to comply with future warnings - Pandas 3.0.   ## v1.0.1 **Minor Bugs Update**  - Fixes instances where flattened correlation clusters were misaligned to structural clusters. - Readme licence badge links directly to GitHub licence file (URL).   ## v1.0.0 **Initial Release**"},{"location":"api_references/MamsiPls/","title":"MamsiPls","text":""},{"location":"api_references/MamsiPls/#mamsipls","title":"MamsiPls","text":"<p>source <pre><code>MamsiPls(\n   n_components = 2, full_svd = False, method = 'NIPALS', standardize = True,\n   max_tol = 1e-14, nipals_convergence_norm = 2, calc_all = True, sparse_data = False,\n   copy = True\n)\n</code></pre></p> <p>A class that extends the MB_PLS class by extra methods convenient in Chemometrics and Metabolomics research.  It is based on MB-PLS package: Baum et al., (2019). Multiblock PLS: Block dependent prediction modeling for Python. This wrapper has some extra methods convenient in Chemometrics and Metabolomics research.</p> <p>For a full list of methods, please refer to the MB-PLS class documentation.</p> <p>Args</p> <ul> <li>n_components (int, optional) : A number of Latent Variables (LV). Defaults to 2.</li> <li>full_svd (bool, optional) : Whether to use full singular value decomposition when performing the SVD method.      Set to False when using very large quadratic matrices (X). Defaults to False.</li> <li>method (str, optional) : The method used to derive the model attributes. Options are 'UNIPALS', 'NIPALS', 'SIMPLS',      and 'KERNEL'. Defaults to 'NIPALS'.</li> <li>standardize (bool, optional) : Whether to standardise the data (Unit-variance scaling). Defaults to True.</li> <li>max_tol (float, optional) : Maximum tolerance allowed when using the iterative NIPALS algorithm. Defaults to 1e-14.</li> <li> <p>nipals_convergence_norm (int, optional) : Order of the norm that is used to calculate the difference of      the super-score vectors between subsequential iterations of the NIPALS algorithm.      Following orders are available:</p> ord norm for matrices norm for vectors None Frobenius norm 2-norm 'fro' Frobenius norm -- 'nuc' nuclear norm -- inf max(sum(abs(x), axis=1)) max(abs(x)) -inf min(sum(abs(x), axis=1)) min(abs(x)) 0 -- sum(x != 0) 1 max(sum(abs(x), axis=0)) as below -1 min(sum(abs(x), axis=0)) as below 2 2-norm (largest sing. value) as below -2 smallest singular value as below other -- sum(abs(x)ord)(1./ord) <p>Defaults to 2.</p> </li> <li> <p>calc_all (bool, optional) : Whether to calculate all internal attributes for the used method. Some methods do not need     to calculate all attributes (i.e., scores, weights) to obtain the regression coefficients used for prediction.     Setting this parameter to False will omit these calculations for efficiency and speed. Defaults to True.</p> </li> <li>sparse_data (bool, optional) : NIPALS is the only algorithm that can handle sparse data using the method of H. Martens     and Martens (2001) (p. 381). If this parameter is set to True, the method will be forced to NIPALS and sparse data     is allowed. Without setting this parameter to True, sparse data will not be accepted. Defaults to False.</li> <li>copy (bool, optional) : Whether the deflation should be done on a copy. Not using a copy might alter the input data     and have unforeseeable consequences. Defaults to True.</li> </ul> <p>Attributes</p> <ul> <li>n_components (int) : Number of Latent Variables (LV).</li> <li>Ts_ (array) : (X-Side) super scores [n,k]</li> <li>T_ (list) : (X-Side) block scores [i][n,k]</li> <li>W_ (list) : (X-Side) block weights [n][pi,k]</li> <li>A_ (array) : (X-Side) block importances/super weights [i,k]</li> <li>A_corrected_ (array) : (X-Side) normalized block importances (see mbpls documentation)</li> <li>P_ (list, block) : (X-Side) loadings [i][pi,k]</li> <li>R_ (array) : (X-Side) x_rotations R=W(PTW)-1</li> <li>explained_var_x_ (list) : (X-Side) explained variance in X per LV [k]</li> <li>explained_var_xblocks_ (array) : (X-Side) explained variance in each block Xi [i,k]</li> <li>beta_ (array) : (X-Side) regression vector \ud835\udefd [p,q]</li> <li>U_ (array) : (Y-Side) scoresInitialize [n,k]</li> <li>V_ (array) : (Y-Side) loadings [q,k]</li> <li>explained_var_y_ (list) : (Y-Side) explained variance in Y [k]</li> </ul> <p>Methods:</p>"},{"location":"api_references/MamsiPls/#estimate_lv","title":".estimate_lv","text":"<p>source <pre><code>.estimate_lv(\n   x, y, groups = None, max_components = 10, classification = True, metric = 'auc',\n   method = 'kfold', n_splits = 5, repeats = 100, test_size = 0.2, random_state = 42,\n   plateau_threshold = 0.01, increase_threshold = 0.05, get_scores = False,\n   savefig = False, n_jobs = -1, **kwargs\n)\n</code></pre></p> <p>A method to estimate the number of latent variables (LVs)/components in the MB-PLS model.    The method is based on cross-validation (k-fold or Monte Carlo) and combined with an outer loop with increasing number of LVs.    LV on which the model stabilises corresponds with the optimal number of LVs.</p> <p>Args</p> <ul> <li>x (array or list['array']) : All blocks of predictors x1, x2, ..., xn. Rows are observations, columns are features/variables.</li> <li>y (array) : A 1-dim array of reference values, either continuous or categorical variable.</li> <li>groups (array, optional) : If provided, cv iterator variant with non-overlapping groups.      Group labels for the samples used while splitting the dataset into train/test set.     Defaults to None.</li> <li>max_components (int, optional) : Maximum number of components for whic LV estimate is calculated.      Defaults to 10.</li> <li>classification (bool, optional) : Whether to perfrom calssification or regression. Defaults to True.</li> <li>metric (str, optional) : Metric to use to estimate the number of LVs; available options: [<code>AUC</code>, <code>precision</code>, <code>recall</code>, <code>f1</code>] for      categorical outcome variables and ['q2'] for continuous outcome variable.      Defaults to 'auc'.</li> <li>method (str, optional) : Corss-validation method. Available options ['kfold', 'montecarlo']. Defaults to 'kfold'.</li> <li>n_splits (int, optional) : Number of splits for k-fold cross-validation. Defaults to 5.</li> <li>repeats (int, optional) : Number of train-test split repeats from Monte Carlo. Defaults to 100.</li> <li>test_size (float, optional) : Test size for Monte Carlo. Defaults to 0.2.</li> <li>random_state (int, optional) : Generates a sequence of random splits to control MCCV. Defaults to 42.</li> <li>plateau_threshold (float, optional) : Maximum increase for a sequence of LVs to be considered a plateau.      Must be non-negative.      Defaults to 0.01.</li> <li>increase_threshold (float, optional) : Minimum increase to be considered a bend. Must be non-negative.. Defaults to 0.05.</li> <li>get_scores (bool, optional) : Whether to retun measured mean scores. Defaults to False.</li> <li>savefig (bool, optional) : Whether to save the plot as a figure. If True, argument <code>fname</code> has to be provided. Defaults to False.</li> <li>n_jobs (int, optional) : Number of workers (CPU cores) for multiprocessing, -1 utilises all available cores on a system.      Defaults to -1.</li> <li>kwargs  : Additional keyword arguments to be passed to plt.savefig(), fname required to save .</li> </ul> <p>Raises</p> <ul> <li>ValueError  : Incorrect metric for categorical outcome. Allowed values are: 'auc', 'precision', 'recall', 'specificity', 'f1', 'accuracy'.</li> <li>ValueError  : Incorrect metric for continuous outcome. Allowed values are: 'q2'.</li> <li>ValueError  : Invalid method. Available options are ['kfold', 'montecarlo'].</li> </ul> <p>Returns</p> <ul> <li>DataFrame  : Measured mean scores for test and train splits for all components.</li> </ul>"},{"location":"api_references/MamsiPls/#evaluate_class_model","title":".evaluate_class_model","text":"<p>source <pre><code>.evaluate_class_model(\n   x, y\n)\n</code></pre></p> <p>Evaluate classification MB-PLS model using a testing dataset.</p> <p>Args</p> <ul> <li>x (array or list[array]) : All blocks of predictors x1, x2, ..., xn. Rows are observations, columns are features/variables.</li> <li>y (array) : 1-dim or 2-dim array of reference values - categorical variable.</li> </ul> <p>Returns</p> <ul> <li>array  : Predicted y variable based on training set predictors.</li> </ul>"},{"location":"api_references/MamsiPls/#evaluate_regression_model","title":".evaluate_regression_model","text":"<p>source <pre><code>.evaluate_regression_model(\n   x, y\n)\n</code></pre></p> <p>Evaluate regression MB-PLS model using a testing dataset.</p> <p>Args</p> <ul> <li>x (array or list[array]) : All blocks of predictors x1, x2, ..., xn. Rows are observations, columns are features/variables.</li> <li>y (array) : 1-dim or 2-dim array of reference values - continuous variable.</li> </ul> <p>Returns</p> <ul> <li>array  : Predicted y variable based on training set predictors.</li> </ul>"},{"location":"api_references/MamsiPls/#kfold_cv","title":".kfold_cv","text":"<p>source <pre><code>.kfold_cv(\n   x, y, groups = None, classification = True, return_train = False, n_splits = 5,\n   n_jobs = -1\n)\n</code></pre></p> <p>Perform k-fold cross-validation for MB-PLS model.</p> <p>Args</p> <ul> <li>x (array or list[array]) : All blocks of predictors x1, x2, ..., xn. Rows are observations, columns are features/variables.</li> <li>y (array) : 1-dim or 2-dim array of reference values, either continuous or categorical variable.</li> <li>groups (array, optional) : Group labels for the samples used while splitting the dataset into train/test set.     If provided, group k-fold is performed.      Defaults to None.</li> <li>classification (bool, optional) : Whether the outcome is a categorical variable. Defaults to True.</li> <li>return_train (bool, optional) : Whether to return evaluation metrics for training set. Defaults to False.</li> <li>n_splits (int, optional) : Number of splits for k-fold cross-validation. Defaults to 5.</li> <li>n_jobs (int, optional) : Number of workers (CPU cores) for multiprocessing, -1 utilises all available cores on a system.      Defaults to -1.</li> </ul> <p>Returns</p> <ul> <li>DataFrame  : Evaluation metrics for each k-fold split.     if return_train is True, returns evaluation metrics for training set as well.</li> </ul>"},{"location":"api_references/MamsiPls/#montecarlo_cv","title":".montecarlo_cv","text":"<p>source <pre><code>.montecarlo_cv(\n   x, y, groups = None, classification = True, return_train = False, test_size = 0.2,\n   repeats = 10, random_state = 42, n_jobs = -1\n)\n</code></pre></p> <p>Evaluate MB-PLS model using Monte Carlo Cross-Validation (MCCV).</p> <p>Args</p> <ul> <li>x (array or list[array]) : All blocks of predictors x1, x2, ..., xn. Rows are observations, columns are featuress.</li> <li>y (array) : 1-dim or 2-dim array of reference values - categorical variable.</li> <li>groups (array, optional) : Group labels for the samples used while splitting the dataset into train/test set.     If provided, group-train-test split will be used instead of train-test split for random splits.      Defaults to None.</li> <li>classification (bool, optional) : Whether the outcome is a categorical variable. Defaults to True.</li> <li>return_train (bool, optional) : Whether to return evaluation metrics for training set. Defaults to False.</li> <li>test_size (float, optional) : Proportion of the dataset to include in the test split. Defaults to 0.2.</li> <li>repeats (int, optional) : Number of MCCV repeats. Defaults to 10.</li> <li>random_state (int, optional) : Generates a sequence of random splits to control MCCV. Defaults to 42.</li> <li>n_jobs (int, optional) : Number of workers (CPU cores) for multiprocessing, -1 utilises all available cores on a system.      Defaults to -1.</li> </ul> <p>Returns</p> <ul> <li>DataFrame  : Evaluation metrics for each MCCV repeat.     if return_train is True, returns evaluation metrics for training set as well.</li> </ul>"},{"location":"api_references/MamsiPls/#mb_vip","title":".mb_vip","text":"<p>source <pre><code>.mb_vip(\n   plot = True, get_scores = False, savefig = False, **kwargs\n)\n</code></pre></p> <p>Multi-block Variable Importance in Projection (MB-VIP) for multiblock PLS model.</p> <p>Adaptation of C. Wieder et al., (2024). PathIntegrate, doi: 10.1371/journal.pcbi.1011814.</p> <p>Args</p> <ul> <li>plot (bool, optional) : Whether to plot MB-VIP scores. Defaults to True.</li> <li>get_scores (bool, optional) : Whether to return MB-VIP scores. Defaults to False.</li> <li>savefig (bool, optional) : Whether to save the plot as a figure. If True, argument <code>fname</code> has to be provided.      Defaults to False.</li> <li>kwargs  : Additional keyword arguments to be passed to plt.savefig(), fname required to save .            </li> </ul> <p>Returns</p> <ul> <li>array  : MB-VIP scores.</li> </ul>"},{"location":"api_references/MamsiPls/#block_importance","title":".block_importance","text":"<p>source <pre><code>.block_importance(\n   block_labels = None, normalised = True, plot = True, get_scores = False,\n   savefig = False, **kwargs\n)\n</code></pre></p> <p>Calculate the block importance for each block in the multiblock PLS model and plot the results.</p> <p>Args</p> <ul> <li>block_labels (list, optional) : List of block names. If block names are not provided or they do not match the number      of blocks in the model, the plot will display labels as 'Block 1', 'Block 2', ... 'Block n'. Defaults to None.</li> <li>normalised (bool, optional) : Whether to use normalised block importance. For more information see model attribute      'A_Corrected_'. Defaults to True.</li> <li>plot (bool, optional) : Whether to render plot block importance. Defaults to True.</li> <li>get_scores (bool, optional) : Whether to return block importance scores. Defaults to False.</li> <li>savefig (bool, optional) : Whether to save the plot as a figure. If True, argument <code>fname</code> has to be provided.      Defaults to False.</li> <li>kwargs  : Additional keyword arguments to be passed to plt.savefig(), fname required to save.</li> </ul> <p>Returns</p> <ul> <li>array  : Block importance scores.</li> </ul>"},{"location":"api_references/MamsiPls/#mb_vip_permtest","title":".mb_vip_permtest","text":"<p>source <pre><code>.mb_vip_permtest(\n   x, y, n_permutations = 1000, return_scores = False, n_jobs = -1\n)\n</code></pre></p> <p>Calculate empirical p-values for each feature by permuting the Y outcome variable <code>n_permutations</code> times and refitting the model. The p-values for each feature are calculated by counting the number of trials with MB-VIP greater than or equal to the observed test statistic, and dividing this by <code>n_permutations</code>.</p> <p>Args</p> <ul> <li>x (array or list[array]) : All blocks of predictors x1, x2, ..., xn. Rows are observations, columns are features/variables.</li> <li>y (array) : 1-dim or 2-dim array of reference values, either continuous or categorical variable.</li> <li>n_permutations (int, optional) : Number of permutation tests. Defaults to 1000.</li> <li>return_scores (bool, optional) : Whether to return MB-VIP scores for each permuted null model. Defaults to False.</li> <li>n_jobs (int, optional) : Number of workers (CPU cores) for multiprocessing, -1 utilises all available cores on a system.      Defaults to -1.</li> </ul> <p>Returns</p> <ul> <li>array  : Returns an array of p-values for each feature. If <code>return_scores</code> is True, then a matrix of MB-VIP scores for each permuted null model is returned as well.</li> </ul>"},{"location":"api_references/MamsiPls/#calculate_ci","title":".calculate_ci","text":"<p>source <pre><code>.calculate_ci(\n   data, ci_level = 0.9, dropna = True\n)\n</code></pre></p> <p>Static Method</p> <p>Calculates mean, margin of error, and confidence interval for each column.</p> <p>Args</p> <ul> <li>data (pd.DataFrame) : The input DataFrame.</li> <li>ci_level (float, optional) : The confidence level (e.g., 0.90, 0.95). Defaults to 0.90.</li> <li>dropna (bool, optional) : Whether to drop rows containing NaNs. Defaults to True.</li> </ul> <p>Returns</p> <ul> <li>DataFrame  : A DataFrame containing the calculated statistics for each column.             If dropna = False, and a column has less than 2 valid values after             dropping NaNs specific to that column, all the result values for that             column will be np.nan.</li> </ul>"},{"location":"api_references/MamsiPls/#group_train_test_split","title":".group_train_test_split","text":"<p>source <pre><code>.group_train_test_split(\n   x, y, gropus = None, test_size = 0.2, random_state = 42\n)\n</code></pre></p> <p>Static Method</p> <p>Split the data into train and test sets based on the groups. The groups are split into train and test sets based on the <code>test_size</code> parameter. The function returns the train and test sets for the predictors and the response variable.</p> <p>Args</p> <ul> <li>x (array or list[array]) : All blocks of predictors x1, x2, ..., xn. Rows are samples, columns are features.</li> <li>y (array) : 1-dim or 2-dim array of reference values, either continuous or categorical variable.</li> <li>groups (array, optional) : Group labels for the samples used while splitting the dataset into train/test set. Defaults to None.</li> <li>test_size (float, optional) : Proportion of the dataset to include in the test split. Defaults to 0.2.</li> <li>random_state (int, optional) : Controls the shuffling applied to the data before applying the split. Defaults to 42.</li> </ul> <p>Returns</p> <ul> <li>tuple  : x_train, x_test, y_train, y_test</li> </ul>"},{"location":"api_references/MamsiStructSearch/","title":"MamsiStructSearch","text":""},{"location":"api_references/MamsiStructSearch/#mamsistructsearch","title":"MamsiStructSearch","text":"<p>source <pre><code>MamsiStructSearch(\n   rt_win = 5, ppm = 15\n)\n</code></pre></p> <p>A class for performing structural search on multi-modal MS data using. The class allows to search for structural signatures in LC-MS data based on their m/z and RT. These structural signatures include isotopologues and adduct patterns.</p> <p>Attributes</p> <ul> <li>assay_links (list) : List of data frames containing links for each assay.</li> <li>intensities (numpy.ndarray) : Array of LC-MS intensity data.</li> <li>rt_win (int) : Retention time tolerance window.</li> <li>ppm (int) : Mass-to-charge ratio (m/z) tolerance in ppm.</li> <li>feature_metadata (pandas.DataFrame) : Data frame containing feature metadata extracted from column names.</li> <li>structural_links (pandas.DataFrame) : Data frame containing structural clusters.</li> </ul> <p>Args</p> <ul> <li>rt_win (int, optional) : Retention time tolerance window. Defaults to 5.</li> <li>ppm (int, optional) : Mass-to-charge ratio (m/z) tolerance in ppm. Defaults to 15.</li> </ul> <p>Methods:</p>"},{"location":"api_references/MamsiStructSearch/#load_msi","title":".load_msi","text":"<p>source <pre><code>.load_msi(\n   df\n)\n</code></pre></p> <p>Imports MSI intensity data and extracts feature metadata from column names.</p> <p>Args</p> <ul> <li>df (pandas.DataFrame) : Data frame with MSI intensity data.<ul> <li>rows: samples</li> <li>columns: features (m/z peaks)     Column names in the format: (m/z) For example: 149.111</li> </ul> </li> </ul>"},{"location":"api_references/MamsiStructSearch/#load_lcms","title":".load_lcms","text":"<p>source <pre><code>.load_lcms(\n   df\n)\n</code></pre></p> <p>Imports LC-MS intensity data and extracts feature metadata from column names.</p> <p>Args</p> <ul> <li>df (pandas.DataFrame) : Data frame with LC-MS intensity data.<ul> <li>rows: samples</li> <li>columns: features (LC-MS peaks). Column names in the format: (AssayName)(RTsec)(m/z)m/z.         For example: HPOS_233.25_149.111m/z</li> </ul> </li> </ul>"},{"location":"api_references/MamsiStructSearch/#get_structural_clusters","title":".get_structural_clusters","text":"<p>source <pre><code>.get_structural_clusters(\n   adducts = 'all', annotate = True\n)\n</code></pre></p> <p>Searches structural signatures in LC-MS data based on their m/z and RT. These structural signatures include  isotopologues, adduct patterns and cross-assay links.</p> <p>Args</p> <ul> <li>adducts (str, optional) : Define what type of adducts to .      Possible values are:<ul> <li>'all': All adducts combinations (based on Fiehn Lab adduct calculator).</li> <li>'most-common': Most common adducts for ESI (based on Waters adducts documentation). Defaults to 'all'.</li> </ul> </li> <li>annotate (bool, optional) : Annotate significant features based on National Phenome Centre RIO data.     Only to be run if the data was analysed by the National Phenome Centre or analysis followed their     conventions and protocols. For more information see https://doi.org/10.1021/acs.analchem.6b01481      or https://phenomecentre.org.     Uses semi-targeted annotations for selected compounds.     Defaults to True.</li> </ul> <p>Returns</p> <ul> <li>list (pandas.DataFrame) : DataFrame of significant features with structural clusters.</li> </ul>"},{"location":"api_references/MamsiStructSearch/#get_correlation_clusters","title":".get_correlation_clusters","text":"<p>source <pre><code>.get_correlation_clusters(\n   flat_method = 'constant', cut_threshold = 0.7, max_clusters = 5,\n   cor_method = 'pearson', linkage_method = 'complete', metric = 'euclidean',\n   **kwargs\n)\n</code></pre></p> <p>Clusters features based on their correlations. The method uses hierarchical clustering to create clusters. To flatten clusters, the method uses either a constant threshold or silhouette score.</p> <p>Args</p> <ul> <li>Flattens clusters based on a constant threshold (cut_threshold).<ul> <li>'silhouette': Flattens clusters based on most optimal silhouette score. Defaults to 'constant'.</li> </ul> </li> <li>cut_threshold (float, optional) : Constant threshold for flattening clusters. Defaults to 0.7.</li> <li>max_clusters (int, optional) : Maximum number of clusters for silhouette method. Defaults to 5.</li> <li>cor_method (str {'pearson', 'kendall', 'spearman'}, optional) : Method for calculation correlations. Defaults to 'pearson'.</li> <li>linkage_method (str, optional) : The linkage criterion determines which distance to use between sets of observation.     The algorithm will merge the pairs of cluster that minimise this criterion.<ul> <li>'single': Single linkage minimises the maximum distance between observations of pairs of clusters.</li> <li>'complete': Complete linkage minimises the maximum distance between observations of pairs of clusters.</li> <li>'average': Average linkage minimises the average of the distances between all observations of pairs of clusters.</li> <li>'ward': Ward minimises the variance of the clusters being merged.</li> <li>'weighted': Weighted linkage minimises the sum of the product of the distances and the number of observations in pairs of clusters.     Only available for 'constant' flatting method.</li> <li>'centroid': Centroid linkage minimises the distance between the centroids of clusters.     Only available for 'constant' flatting method.</li> <li>'median': Median linkage minimises the distance between the medians of clusters.     Only available for 'constant' flatting method. Defaults to 'complete'.</li> </ul> </li> <li>metric (str, optional) : The distance metric to use. The metric to use when calculating distance between instances in a feature array.     Metric used to compute the linkage. Can be \u201ceuclidean\u201d, \u201cl1\u201d, \u201cl2\u201d, \u201cmanhattan\u201d, \u201ccosine\u201d, or \u201cprecomputed\u201d.     If linkage is \u201cward\u201d, only \u201ceuclidean\u201d is accepted. If \u201cprecomputed\u201d, a distance matrix is needed as input for the fit method.     Defaults to 'euclidean'. flat_method (str {'constant', 'silhouette'}, optional):     Method for cluster flattening:</li> </ul>"},{"location":"api_references/MamsiStructSearch/#get_structural_network","title":".get_structural_network","text":"<p>source <pre><code>.get_structural_network(\n   include_all = False, interactive = False, return_nx_object = False,\n   output_file = 'interactive.html', labels = False, master_file = None\n)\n</code></pre></p> <p>Generates a structural network graph based on the provided master file or the loaded structural links data. The method creates a network graph based on the generated structural data or the provided master file. The network graph includes nodes representing features and edges representing different types of links. The graph can be displayed interactively using pyvis.network or using NetworkX and matplotlib. The graph can be saved as a NetworkX object if return_nx_object is True.</p> <p>Args</p> <ul> <li>include_all (bool, optional) : Whether to include all features in the network, even if they are not structurally linked to other features.     Defaults to False.</li> <li>interactive (bool, optional) : Whether to display the network graph interactively using pyvis.network.     If False, the network graph is displayed using NetworkX and Matplotlib.     Defaults to False.</li> <li>return_nx_object (bool, optional) : Whether to return the NetworkX object representing the network graph edited in CytoScape.      Defaults to False.</li> <li>output_file (str, optional) : The name of the output file when displaying the network graph interactively using pyvis.network.     Only applicable when interactive is True.      Defaults to 'interactive.html'.</li> <li>labels (bool, optional) : Whether to display labels for the nodes in the network graph.     Only applicable when interactive is False.      Defaults to False.</li> <li>master_file (pd.DataFrame, optional) : The master file containing necessary columns for generating the network.     This is intended for cases when structural links required manual curation (e.g. manually assigned isotopologue groups, adduct groups, etc.)     If not provided, the function uses the loaded structural links data.     Required columns: <ul> <li>'Feature': Feature ID (e.g. HPOS_233.25_149.111m/z). Required.</li> <li>'Assay': Assay name (e.g. HPOS). Required.</li> <li>'Isotopologue group': Grouped features with similar isotopologue patterns (e.g. 1, 2 ... N). Required.</li> <li>'Isotopologue pattern': Representing M+0, M+1, M+2 ... M+N for each isotopologue group (e.g. 0, 1, 2 ... N). Required.</li> <li>'Adduct group': Grouped features with similar adduct patterns (e.g. 1, 2 ... N). Required.</li> <li>'Adduct': Assigned adduct labels to all features within an adduct group (e.g. [M+H]+, [M-H]-). Required.</li> <li>'Structural cluster': Grouped features based on the intersection of isotopologue and adduct clusters (e.g. 1, 2, 3 ... N). Required.</li> <li>'Correlation cluster': Grouped flattened correlation clusters (e.g. 1, 2, 3 ... N). Required.</li> <li>'Cross-assay link': Linked structural clusters between different assays. Required.</li> <li>'cpdName': Compound name (e.g. Caffeine). Optional. Defaults to None.</li> </ul> </li> </ul> <p>Returns</p> <ul> <li>None  : The NetworkX object representing the network graph, if return_nx_object is True.     Edge weights represent the type of link between features:<ul> <li>Isotopologue: 1</li> <li>Adduct: 5</li> <li>Cross-assay link: 10 Otherwise, None.</li> </ul> </li> </ul> <p>Raises</p> <ul> <li>RuntimeWarning  : If no data is loaded and no master file is provided.</li> <li>RuntimeWarning  : If the provided master file is missing necessary columns.</li> </ul>"}]}